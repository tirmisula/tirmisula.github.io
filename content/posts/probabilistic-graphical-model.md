---
author: "X. Wang"
title: "Probabilistic Graphical Model"
date: "2023-09-12"
description: "A brief introduction."
tags: ["machine learning"]
categories: ["themes", "syntax"]
# series: ["Themes Guide"]
aliases: ["migrate-from-jekyl"]
math: true
ShowBreadCrumbs: false
ShowToc: true
TocOpen: true
---

:                                                         

{{< math.inline >}}
{{ if or .Page.Params.math .Site.Params.math }}

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">

<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>

<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
{{ end }}

{{ if .Page.Store.Get "hasMermaid" }}
  <script type="module">
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs';
    mermaid.initialize({ startOnLoad: true });
  </script>
{{ end }}
{{</ math.inline >}}

<style>
    /* Set the font size of all math elements to 16px */
    .katex {
        font-size: 16px !important;
    }
</style>

<style>
/* Custom CSS styles */
.graph {
    background-color: white;
  /* padding: 10px; */
  /* border-radius: 5px; */
}
.graph pre {
    background-color: white;
  /* font-family: 'Courier New', monospace;
  font-size: 14px;
  line-height: 1.5; */
}
</style>

## Background

### Overview of PGM

<cite>[^1]</cite>

$$
\text{PGM}: \begin{cases}
    \text{Representation} \begin{cases}
        \text{directed graph}\rarr  \text{Bayesian network} \\\
        \text{undirected graph}\rarr \text{Markov network(MRF)} \\\
        \text{continous variable}\rarr \text{Gaussian BN/Gaussian MRF}
    \end{cases} \\\
    \text{Inference} \begin{cases}
        \text{MAP inference$\rarr \hat{x_A}=\argmax_{x_A}p(x_A|x_B)\propto\argmax p(x_A,x_B)$} \\\
        \text{exact inference} \begin{cases}
          \text{Variable elimination(VE)} \\\
          \text{Belief propagation(BP)$\rarr$sum-product algorithm(Tree structure)} \\\
          \text{Junction tree algorithm(Normal graph)}
        \end{cases} \\\
        \text{approximate inference} \begin{cases}
            \text{Loop belief propagation(Cyclic graph)} \\\
            \text{Variational inference} \\\
            \text{MCMC: importance sampling}
        \end{cases} 
    \end{cases} \\\
    \text{Learning} \begin{cases}
        \text{parameter learning} \begin{cases}
            \text{complete data: $(x,z)$} \\\
            \text{hidden variable: $z$}
        \end{cases} \\\
        \text{structure learning}
    \end{cases}
\end{cases}
$$

### Operations for high dimensional data

$$
\begin{cases}
\text{sum rule: } p(x_1)=\int p(x_1,x_2)dx_2 \\\
\text{product rule: } p(x_1,x_2)=p(x_1)P(x_2|x_1) \\\
\text{chain rule: } p(x_1,\cdots,x_p)=p(x_1|x_2,...,x_p) \cdots p(x_{p-2}|x_{p-1},x_p)p(x_{p-1}|x_p)p(x_p) \\\
\text{bayesian rule: } p(x_2|x_1)=\frac{p(x_1,x_2)}{p(x_1)}=\frac{p(x_1,x_2)}{\int p(x_1,x_2)dx_2}=\frac{p(x_1,x_2)}{\int p(x_2)p(x_1|x_2)dx_2}
\end{cases}
$$

### Simplifications for high dimensional data

Considering high computation cost, high dimensional data can be simplified at different level:

$$
\begin{align*}
&1. \text{ Each dimension is independent} \rarr p(x|y)=\prod_{i=1}^p p(x_i|y) \text{ (Naive Bayes)} \\\
&2. \text{ Conditionally independent} \rarr x_j\perp x_{i+1}|x_i, j<i \text{(Hidden Markov)} \\\
&3. \text{ Conditionally independent in general}\rarr x_A\perp x_{B}|x_C \text{( $x_A,x_B,x_C$ are dimensions set)} \\\
&\text{e.g. }x_j\perp x_{i}|x_{i-1},x_{i-3}
\end{align*}
$$

## Bayesian network
### Definition

Undirected acyclic Bayesian network graph is generated by <b>toposort</b> following the rule below:

$$
\text{Given: } a|b \text{, the graph has partial structure: }
$$

<!-- ```goat
 .-.            .-.           
| 1 |<---  | 2 |
 '-'           '-'
``` -->

<!-- add html template to change bg color -->
<!-- ```html

<div class="graph" style="text-align: center;">

```mermaid
flowchart LR

```

</div>

``` -->

<div class="graph" style="text-align: center;">

```mermaid
%%{
  init: {
    'theme': 'base',
    'themeVariables': {
      'primaryColor': 'white',
      'primaryTextColor': '#000',
      'primaryBorderColor': '#7C0200',
      'lineColor': '#F8B229',
      'secondaryColor': 'red',
      'tertiaryColor': '#fff'
    }
  }
}%%
flowchart LR
    id1((b)) --> id2((a))
```

</div>

Bayesian network has 2 attributes: **conditionally independent** and **factorization**:

$$
\begin{align*}
&1. \text{ Conditionally independent: }x_A\perp x_{B}|x_C \\\
&2. \text{ Factorization: }p(x_1,\cdots,x_p)=\prod_{i=1}^p p(x_i|x_{pa(i)}), \text{where }x_{pa(i)}\text{ is the set of $x_i$'s parents}
\end{align*}
$$

### Local features of BN

There are three partial graph structures:

#### tail to tail

<div class="graph" style="text-align: center;">

```mermaid
%%{
  init: {
    'theme': 'base',
    'themeVariables': {
      'primaryColor': 'white',
      'primaryTextColor': '#000',
      'primaryBorderColor': '#7C0200',
      'lineColor': '#F8B229',
      'secondaryColor': 'red',
      'tertiaryColor': '#fff'
    }
  }
}%%
flowchart LR
    id1((a)) --> id2((b))
    id1((a)) --> id3((c))
```

</div>

Based on factorization, we have:

$$
\begin{align*}
p(a,b,c) = p(a)p(b|a)p(c|a)
\end{align*}
$$

Based on chain rule, we have:

$$
\begin{align*}
p(a,b,c) = p(a)p(b|a)p(c|a,b)
\end{align*}
$$

So,

$$
\begin{align*}
p(a)p(b|a)p(c|a) &= p(a)p(b|a)p(c|a,b) \\\
p(c|a) &= p(c|a,b) \\\
c\perp b | a, &\text{c is not affected whether $b$ is observed}
\end{align*}
$$

{{< math.inline >}}
<p>
So we can conclude that while \(a\) is observed, the path from \(b\) to \(c\) is blocked so \(b\) and \(c\) are mutually independent.
</p>
{{</ math.inline >}}

#### head to tail

<div class="graph" style="text-align: center;">

```mermaid
%%{
  init: {
    'theme': 'base',
    'themeVariables': {
      'primaryColor': 'white',
      'primaryTextColor': '#000',
      'primaryBorderColor': '#7C0200',
      'lineColor': '#F8B229',
      'secondaryColor': 'red',
      'tertiaryColor': '#fff'
    }
  }
}%%
flowchart LR
    id1((a)) --> id2((b))
    id2((b)) --> id3((c))
```

</div>

Based on factorization, we have:

$$
\begin{align*}
p(a,b,c) = p(a)p(b|a)p(c|b)
\end{align*}
$$

Based on chain rule, we have:

$$
\begin{align*}
p(a,b,c) = p(a)p(b|a)p(c|a,b)
\end{align*}
$$

So,

$$
\begin{align*}
p(a)p(b|a)p(c|b) &= p(a)p(b|a)p(c|a,b) \\\
p(c|b) &= p(c|a,b) \\\
c\perp a | b, &\text{c is not affected whether $a$ is observed}
\end{align*}
$$

{{< math.inline >}}
<p>
So we can conclude that while \(b\) is observed, the path from \(a\) to \(c\) is blocked so \(a\) and \(c\) are mutually independent.
</p>
{{</ math.inline >}}

#### head to head

<div class="graph" style="text-align: center;">

```mermaid
%%{
  init: {
    'theme': 'base',
    'themeVariables': {
      'primaryColor': 'white',
      'primaryTextColor': '#000',
      'primaryBorderColor': '#7C0200',
      'lineColor': '#F8B229',
      'secondaryColor': 'red',
      'tertiaryColor': '#fff'
    }
  }
}%%
flowchart LR
    id1((a)) --> id3((c))
    id2((b)) --> id3((c))
```

</div>

Based on factorization, we have:

$$
\begin{align*}
p(a,b,c) = p(a)p(b)p(c|a,b)
\end{align*}
$$

Based on chain rule, we have:

$$
\begin{align*}
p(a,b,c) = p(a)p(b|a)p(c|a,b)
\end{align*}
$$

So,

$$
\begin{align*}
p(a)p(b)p(c|a,b) &= p(a)p(b|a)p(c|a,b) \\\
p(b) &= p(b|a) \\\
b\perp a , &\text{$b$ does not affect $a$'s happening}
\end{align*}
$$

{{< math.inline >}}
<p>
So we can conclude that \(a\) and \(b\) are mutually independent if \(c\) is not observed, if \(c\) is observed \(a\) and \(b\) are related.
</p>
{{</ math.inline >}}

{{< math.inline >}}
<p>
<mark>It also applies</mark> when \( c \) has child node \( d \):
</p>
{{</ math.inline >}}

$$
\begin{align*}
p(a)p(b)p(c|a,b)p(d|c) &= p(a)p(b|a)p(c|a,b)p(d|a,b,c) \\\
p(a)p(b)p(c,d|a,b) &= p(a,b,c,d) \\\
p(a)p(b)p(c,d|a,b) &= p(a,b)p(c,d|a,b) \\\
p(a)p(b) &= p(a,b) \\\
b&\perp a
\end{align*}
$$

### D-separation
#### the rules of d-separation
{{< math.inline >}}
<p>
D-separation is a method to check whether a Bayesian network satisfies conditional independent, so that:
</p>
{{</ math.inline >}}

$$
\begin{align*}
x_A \perp x_C | x_B
\end{align*}
$$

{{< math.inline >}}
<p>
The rules are:
</p>
{{</ math.inline >}}

$$
\begin{align*}
&1. \text{If node $\alpha \in $tail to tail(parent node), $\alpha \in x_B$}\\\
&2. \text{If node $\alpha \in $head to tail(intermediate node), $\alpha \in x_B$}\\\
&3. \text{If node $\alpha \in $head to head(child node OR descendant of child node), $\alpha \notin x_B$}\\\
\end{align*}
$$

The rule of D-separation is also called <b>global markov property</b>.

#### markov blanket

Given a conditinal probability:

$$
\begin{align*}
p(x_i|x_{\neq i}) &= \frac{p(x_i,x_{\neq i})}{p(x_{\neq i})} \\\
&= \frac{p(x)}{\int p(x)dx_i} \\\
&\because p(x) = \prod_{j=1}^p p(x_j|x_{pa(j)}) = f(x_i)g(x_{\neq i}) \\\
&\text{$p(x)$ can be separated to 2 parts: $f(x_i)$ related to $x_i$, $g(x_{\neq i})$ not related to $x_i$} \\\
&= \frac{f(x_i)g(x_{\neq i})}{g(x_{\neq i})\int f(x_i)dx_i} \\\
&= \frac{f(x_i)}{\int f(x_i)dx_i}
\end{align*}
$$

{{< math.inline >}}
<p>
It gives us information: conditional probability of \(x_i\) is affected only by the local area of node \(x_i\), not all nodes from global, This covered area is called Markov blanket which includes:
</p>
{{</ math.inline >}}

$$
f(x_i):\begin{cases}
p(x_i|x_{pa(i)}) & \text{$x_i$'s parent nodes} \\\
p(x_{child(i)}|x_i, x_{pa(x_{child(i)})}) & \text{$x_i$'s child nodes and $x_i$'s spouse nodes}
\end{cases}
$$

<div style="text-align: center;">

```mermaid
%%{
  init: {
    'theme': 'base',
    'themeVariables': {
      'primaryColor': 'white',
      'primaryTextColor': '#000',
      'primaryBorderColor': '#7C0200',
      'lineColor': '#F8B229',
      'secondaryColor': 'red',
      'tertiaryColor': '#fff'
    }
  }
}%%
flowchart TB
    subgraph Markov blanket
      id1((pa)) --> id3((x))
      id2((pa)) --> id3((x))
      id3((x)) --> id4((child))
      id3((x)) --> id5((child))
      id6((sp)) --> id4((child))
      id7((sp)) --> id5((child))
    end
    id1((pa)) -.-> id8((...))
    id2((pa)) -.-> id9((...))
    id10((...)) -.-> id6((sp))
```

</div>

### BN models

$$
\text{BN models}: \begin{cases}
    \text{Singular: Naive Bayes} \\\
    \text{Mixture: Gaussian Mixture Model} \\\
    \text{Stochastic process: } \begin{cases}
        \text{Markov chain} \\\
        \text{Gaussian process}
    \end{cases} \\\
    \text{Continous: Gaussian Bayesian Network}
\end{cases}
$$

## Markov random network
### Conditional independence of MRF

The conditional independence is shown in 3 ways:

1. global markov property

$$
x_A\perp x_C | x_B
$$

2. local markov property

$$
x\perp \lbrace \mathbb{U}-x-\lbrace\text{neighbor of $x$}\rbrace \rbrace | \text{neighbor of $x$} \\\
\text{e.g. $a\perp \lbrace d,f \rbrace | \lbrace b,c,e \rbrace$ for the following graph} \\\
\dArr
$$

<div class="graph" style="text-align: center;">

```mermaid
%%{
  init: {
    'theme': 'base',
    'themeVariables': {
      'primaryColor': 'white',
      'primaryTextColor': '#000',
      'primaryBorderColor': '#7C0200',
      'lineColor': '#F8B229',
      'secondaryColor': 'red',
      'tertiaryColor': '#fff'
    }
  }
}%%
flowchart TB
    id1((a)) --- id2((b))
    id1((a)) --- id3((c))
    id2((b)) --- id4((d))
    id1((a)) --- id5((e))
    id5((e)) --- id6((f))
```

</div>

3. pairwise markov property

$$
x_i\perp x_j | x_{\neq i, \neq j}
$$

Three properties are <b>equivalent</b>:

$$
\text{global markov $\iff$ local markov $\iff$ pairwise markov}
$$

{{< math.inline >}}
<p>
From pairwise markov, we know that if \(x_i \perp x_j\), other nodes are blocked. It means \(x_i\) and \(x_j\) are not connected, \(x_i\) and \(x_j\) destinedly belongs to different <b>max-clique</b>.
</p>
{{</ math.inline >}}

$$
\begin{align*}
\textbf{clique: }&\text{nodes in clique are fully connected} \\\
\textbf{maximum clique: }&\text{the clique that can cover maximum number of nodes}
\end{align*}
$$

### Factorization of MRF

According to **Hammersley–Clifford theorem**<cite>[^2]</cite>, factorization of undirected graph MRF can be expressed as production of potential function on maximum clique:

$$
\begin{align*}
p(x) &= \frac{1}{z} \prod_{i=1}^K \psi(x_{C_i}) \\\
C_i &: \text{i-th maximum clique} \\\
x_{C_i} &: \text{variable nodes in $C_i$} \\\
\psi(x) &: \text{potential function, $>0$} \\\
z &: \text{nomalize factor, $z=\sum_{x_1\cdots x_p}\prod_{i=1}^K \psi(x_{C_i})$}
\end{align*}
$$

To be greater than zero, potential function has exponential form:

$$
\begin{align*}
\psi(x_{C_i}) &= \exp(-E(x_{C_i})) \\\
E(x) &: \text{energy function}
\end{align*}
$$

{{< math.inline >}}
<p>
For such pdf \( p(x) \) made up of potential functions, we call it <b>Gibbs distribution</b> or <b>Boltzman distribution</b>, Gibbs distirbution is also exponential family distribution:
</p>
{{</ math.inline >}}

$$
\begin{align*}
p(x) &= \frac{1}{z}\prod_{i=1}^K\exp(-E(x_{C_i})) \\\
&= \frac{1}{z} \exp( -\sum_{i=1}^K E(x_{C_i})) \\\
&= \frac{1}{z(\eta)}h(x)\exp( \eta^T\phi(x) )
\end{align*}
$$

Because Gibbs distribution belongs to exponential family distribution, it maximize entropy naturally, thus we can conclude:

$$
\begin{align*}
\text{MRF distribution}\iff \text{Gibbs distribution}&\iff \text{Distribution that maximize entropy} \\\ 
&\iff \text{most likely to be observed}
\end{align*}
$$

## Inference in PGM
### Variable elimination

VE thought is based on distirbutive law:

$$
a(b+c) \text{ is better than } ab+ac \\\
\text{summation is better than multiplication}
$$

Suppose we have directed graph:

<div class="graph" style="text-align: center;">

```mermaid
%%{
  init: {
    'theme': 'base',
    'themeVariables': {
      'primaryColor': 'white',
      'primaryTextColor': '#000',
      'primaryBorderColor': '#7C0200',
      'lineColor': '#F8B229',
      'secondaryColor': 'red',
      'tertiaryColor': '#fff'
    }
  }
}%%
flowchart LR
    id1((a)) --> id2((b))
    id2((b)) --> id3((c))
    id3((c)) --> id4((d))
```

</div>

We can compute marginal probability by:

$$
\begin{align*}
p(d) &= \sum_{a,b,c} p(a,b,c,d) \\\
&= \sum_{a,b,c} p(a)p(b|a)p(c|b)p(d|c) \\\
\end{align*}
$$

{{< math.inline >}}
<p>
The computation complexity is \(O(4^k)\). we can turn it to:
</p>
{{</ math.inline >}}

$$
\begin{align*}
p(d) &= \sum_{a,b,c} p(a)p(b|a)p(c|b)p(d|c) \\\
&= \sum_{b,c}p(c|b)p(d|c)\sum_{a}\underset{f(a,b)}{p(a)p(b|a)} \\\
&= \sum_{c}p(d|c)\sum_{b}p(c|b)f(b) \\\
&= \sum_{c}p(d|c)f(c) \\\
&= f(d)
\end{align*}
$$

{{< math.inline >}}
<p>
It is the same for undirected graph:
</p>
{{</ math.inline >}}

$$
\begin{align*}
p(d) &= \sum_{a,b,c} p(a,b,c,d) \\\
&= \sum_{a,b,c} \frac{1}{z}\prod_{i=1}^k \psi(x_{C_i}) \\\
&\text{the worst case is $a\in\forall x_{C}$}
\end{align*}
$$

The <b>drawbacks</b> are:

1. Only one be calculated at a time, the intermediate results are not in storage which will be calculated repeatedly.
2. Eliminate order affects computation comlexity, finding optimal eliminate order is NP-hard.

### Repeat computation in VE
#### Example of repeat computation
Repeat computation, for example:

<div class="graph" style="text-align: center;">

```mermaid
%%{
  init: {
    'theme': 'base',
    'themeVariables': {
      'primaryColor': 'white',
      'primaryTextColor': '#000',
      'primaryBorderColor': '#7C0200',
      'lineColor': '#F8B229',
      'secondaryColor': 'red',
      'tertiaryColor': '#fff'
    }
  }
}%%
flowchart LR
    id1((a)) --> id2((b))
    id2((b)) --> id3((c))
    id3((c)) --> id4((d))
    id4((d)) --> id5((e))
```

</div>

{{< math.inline >}}
<p>
We can compute \(p(e)\) and \(p(c)\);
</p>
{{</ math.inline >}}

$$
\begin{align*}
p(e) &= \sum_d p(e|d) \sum_c p(d|c) \sum_b p(c|b) \sum_a p(b|a)p(a) \\\
&\text{Elimination order is $a\rarr b\rarr c \rarr d\rarr e$} \\\
p(c) &= \sum_b p(c|b) \sum_a p(b|a)p(a) \sum_d p(d|c) \sum_e p(e|d) \\\
&\text{Elimination order is $a\rarr b\rarr c \larr d\larr e$} \\\
\end{align*}
$$

{{< math.inline >}}
<p>
It is obvious that the repeated part is \( a\rarr b\rarr c \) which is \( \sum_b p(c|b) \sum_a p(b|a)p(a) \), we can denote this part as:
</p>
{{</ math.inline >}}

$$
\begin{align*}
f_{a\rarr b} (x_b) &= \sum_a p(b|a)p(a) \\\
f_{b\rarr c} (x_c) &= \sum_b p(c|b)f_{a\rarr b} (x_b)
\end{align*}
$$

#### Solve repeat computation

Suppose we have undirected graph:

<div class="graph" style="text-align: center;">

```mermaid
%%{
  init: {
    'theme': 'base',
    'themeVariables': {
      'primaryColor': 'white',
      'primaryTextColor': '#000',
      'primaryBorderColor': '#7C0200',
      'lineColor': '#F8B229',
      'secondaryColor': 'red',
      'tertiaryColor': '#fff'
    }
  }
}%%
flowchart LR
    id1((a)) --- id2((b))
    id2((b)) --- id3((c))
    id2((b)) --- id4((d))
    id1((a)) --- id5((e))
```

</div>

The joint probability is:

$$
p(a,b,c,d,e) = \frac{1}{z}\psi(a)\psi(b)\psi(c)\psi(d)\psi(e)\psi(a,b)\psi(b,c)\psi(b,d)\psi(a,e)
$$

{{< math.inline >}}
<p>
The marginal probability \(p(a)\) is:
</p>
{{</ math.inline >}}

$$
\begin{cases}
p(a) = \psi(a)f_{b\rarr a}(x_a)f_{e\rarr a}(x_a) \\\
f_{b\rarr a}(x_a) = \sum_b \psi(a,b)\psi(b)f_{c\rarr b}(x_b)f_{d\rarr b}(x_b) \\\
f_{e\rarr a}(x_a) = \sum_e \psi(a,e)\psi(e)
\end{cases}
$$

A more <b>general form</b>:

$$
\forall i \in \text{vertices in $G(V,E)$} \\\
\begin{cases}
p(i) = \psi(i)\prod_{j\in neighbor(i)} f_{j\rarr i}(x_i) \\\
f_{j\rarr i}(x_i) = \sum_{j} \psi(i,j)\psi(j) \prod_{k\in neighbor(j)} f_{k\rarr j}(x_j)
\end{cases}
$$

Visualizing the general form:

<div class="graph" style="text-align: center;">

```mermaid
%%{
  init: {
    'theme': 'base',
    "htmlLabels": true,
    "securityLevel": "loose",
    'themeVariables': {
      'primaryColor': 'white',
      'primaryTextColor': '#000',
      'primaryBorderColor': '#7C0200',
      'lineColor': '#F8B229',
      'secondaryColor': 'red',
      'tertiaryColor': '#fff'
    }
  }
}%%
flowchart LR
    id1((p(i))) ---|f_j→i| id2((j))
    id1((p(i))) ---|f_j→i| id3((j))
    id1((p(i))) ---|f_j→i| id4((...))
    id1((p(i))) ---|f_j→i| id5((j))
    id1((p(i))) --- id6(("&psi;(i)"))
```

</div>

## Conclusion

{{< math.inline >}}
<p>
<b>MCMC</b>: Based on the fact that Markov chain converges to stationary distribution after enough transitions. </br>
1. Design transition matrix \(Q\) <br/>
2. When Markov chain converges the stationary distribution \(q(x)\) \(\approx\) real distribution \(p(x)\) </br>
3. Sample on \(q(x)\)
</p>
{{</ math.inline >}}

<b>Challenge that MCMC facing</b>

1. Though convergence of Markov chain is theoretically proved but we don't know how long it takes to converge.
2. If data is high dimensional and dimension is related to other dimensions it may cause long mixing time or mixing failure. e.g. Gaussian mixture distribution, samples are tend to concentrate on one peak while ignoring other peaks, because transfer from one peak to another through the valley is low probable.
3. Each sampled data is related to previous sampled data.

## Reference

[^1]: From [video](https://www.bilibili.com/video/BV1aE411o7qd?p=46).
[^3]: From [The Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf).
[^5]: From [Mean field variational inference](https://mbernste.github.io/files/notes/MeanFieldVariationalInference.pdf).
[^4]: From [Ross, Sheldon M. (2019). Introduction to probability models](https://doi.org/10.1016%2FC2017-0-01324-1).
[^2]: From [Hammersley–Clifford theorem](http://www.statslab.cam.ac.uk/~grg/books/hammfest/hamm-cliff.pdf).
